{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825dd5f7",
   "metadata": {},
   "source": [
    "# Lecture 6: Model Performance Evaluation\n",
    "\n",
    "Welcome to this hands-on tutorial on **model performance evaluation**! While the theoretical foundations of evaluation metrics are covered in the lectures, this notebook focuses on practical implementations and critical insights for validating your models effectively.\n",
    "\n",
    "In this module, we’ll explore techniques for evaluating the performance of both classification and segmentation models. However, the broader goal is to encourage a **critical** approach to model validation, applicable across a variety of tasks. A robust evaluation framework ensures that your model's predictions are not only accurate but also reliable and generalizable.\n",
    "\n",
    "By the end of this notebook, you’ll understand how to:\n",
    "\n",
    "1) Implement key metrics for classification tasks, such as accuracy, precision, recall, and F1-score.\n",
    "2) Evaluate segmentation models using metrics like Intersection over Union (IoU) and Dice Coefficient.\n",
    "3) Identify and avoid common pitfalls in model evaluation to ensure unbiased and meaningful results.\n",
    "\n",
    "For deeper insights, we recommend exploring these resources:\n",
    "\n",
    "- [Metrics reloaded: recommendations for image analysis validation](https://www.nature.com/articles/s41592-023-02151-z)\n",
    "- [Understanding metric-related pitfalls in image analysis validation](https://www.nature.com/articles/s41592-023-02150-0)\n",
    "\n",
    "Let’s dive in and build a solid foundation for evaluating your models effectively!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e40969",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be8f839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import \\\n",
    "    train_test_split, \\\n",
    "    cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.metrics import \\\n",
    "    confusion_matrix, \\\n",
    "    precision_score, \\\n",
    "    recall_score, \\\n",
    "    f1_score, \\\n",
    "    roc_curve, \\\n",
    "    RocCurveDisplay, \\\n",
    "    roc_auc_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463d4a5",
   "metadata": {},
   "source": [
    "In this section of the notebook, we'll leverage the MNIST dataset. This dataset comprises handwritten digits ranging from 0 to 9, which our model will classify. We'll swiftly train a model and start our evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', data_home=\"./data\", as_frame=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size=1./7., random_state=0)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Label: %i\\n' % int(label), fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45c793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = model.score(X_test, y_test)\n",
    "print('accuracy = ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4214f41d",
   "metadata": {},
   "source": [
    "Great, our model has been trained, and it's yielding a high accuracy. However, accuracy alone doesn't provide the complete picture. Can you determine which classes are the easiest or the most challenging for our model? It's not evident solely from accuracy. Let's delve deeper into performance evaluation to gain a more comprehensive understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffdf4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aff7c2",
   "metadata": {},
   "source": [
    "**Q.1** Can you identify the classes that are challenging to predict? Additionally, could you provide insights into why you think these particular classes are likely the most difficult for our model to classify accurately?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e149719",
   "metadata": {},
   "source": [
    "Consider we just want a model that classifies '9'. Changing the problem to a binary classification model, each images is either a '9' or '0-8'. This results in a massive class inbalance in your data, the accuracy will be mainly dominated by the '0-8' class instead of the '9' class. So this metric doesn't tell you enough about the models ability to identify '9'. Therefor we look at the Receiver Operating Characteristic curve and calculate the Area Under the Curve (AUC) as a metric of the model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fc2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test, y_scores, multi_class=\"ovr\", average=\"macro\")\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c6dcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "\n",
    "class_of_interest = \"9\"\n",
    "class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "\n",
    "display = RocCurveDisplay.from_predictions(\n",
    "    y_onehot_test[:, class_id],\n",
    "    y_scores[:, class_id],\n",
    "    name=f\"{class_of_interest} vs the rest\",\n",
    "    color=\"darkorange\",\n",
    "    plot_chance_level=True,\n",
    ")\n",
    "_ = display.ax_.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"One-vs-Rest ROC curves:\\n9 vs 0-8\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7006e67",
   "metadata": {},
   "source": [
    "Now, consider a scenario where our dataset is very small. How can we obtain a reliable estimate of the model's performance? We will employ cross-validation. The implementation of this technique is demonstrated in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e124e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "scores = cross_val_score(model, mnist.data[:num_samples], mnist.target[:num_samples], cv=5)\n",
    "print(f\"Cross-validation scores: \", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3067bcf",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Q2.** When utilizing 500 data samples, how do the outcomes differ between employing 2-fold cross-validation and 5-fold cross-validation?\n",
    "\n",
    "**Q3.** As your dataset expands, do you typically augment the number of folds for internal validation, or is it more common to decrease them? What are the factors influencing this decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf6241f",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "We won't be training a segmentation model in this instance. However, let's consider an imaginary ground truth representing cancer cells on a histopathology slice. In addition to this ground truth, we have two predictions from different models attempting to segment the cancerous cells. Refer to the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a67be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the reference mask\n",
    "mask1 = np.zeros((16, 16), dtype=int)\n",
    "mask1[2:9, 2:9] = 1 \n",
    "mask1[4:6, 11:14] = 1\n",
    "mask1[10:13, 10:13] = 1\n",
    "\n",
    "# first prediction\n",
    "mask2 = np.zeros((16, 16), dtype=int)\n",
    "mask2[2:8, 2:10] = 1  \n",
    "\n",
    "# second prediction\n",
    "mask3 = np.zeros((16, 16), dtype=int)\n",
    "mask3[1:7, 2:7] = 1 \n",
    "mask3[5:6, 12:13] = 1\n",
    "mask3[10:12, 11:13] = 1\n",
    "\n",
    "# Plot the masks\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(mask1, cmap='Blues', alpha=0.5)\n",
    "plt.title('Ground Truth')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mask2, cmap='Reds', alpha=0.9)\n",
    "plt.imshow(mask1, cmap='Blues', alpha=0.3)\n",
    "plt.title('Prediction 1')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mask3, cmap='Greens', alpha=0.9)\n",
    "plt.imshow(mask1, cmap='Blues', alpha=0.3)\n",
    "plt.title('Prediction 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c3a16a",
   "metadata": {},
   "source": [
    "Try to implement the Dice score and see which model performs best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668e18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # to do: implement the dice score \n",
    "def dice_score(mask1, mask2):\n",
    "    \"\"\"\n",
    "    Calculate the Dice score between two binary masks.\n",
    "\n",
    "    Parameters:\n",
    "        mask1 (ndarray): The first binary mask.\n",
    "        mask2 (ndarray): The second binary mask.\n",
    "\n",
    "    Returns:\n",
    "        float: The Dice score between the two masks.\n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_score_mask1_mask2 = dice_score(mask1, mask2)\n",
    "print(\"Dice score between Ground truth and Prediction 1:\", round(dice_score_mask1_mask2, 2))\n",
    "\n",
    "dice_score_mask1_mask3 = dice_score(mask1, mask3)\n",
    "print(\"Dice score between Ground truth and Prediction 2:\", round(dice_score_mask1_mask3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42e091",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Q3.** Based on the Dice score calculation, which model demonstrates superior performance, and what factors contribute to this result?\n",
    "\n",
    "**Q4.** Beyond the Dice score, what additional metrics should be considered to gain a comprehensive understanding of model performance?\n",
    "\n",
    "**Q5.** Do you stick with your choice or do you maybe thing the other model is better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
