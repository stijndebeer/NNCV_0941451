{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff77e435",
   "metadata": {},
   "source": [
    "# Lecture 3: Convolutional Neural Networks and Segmentation & Detection\n",
    "Welcome to this hands-on tutorial on **Convolutional Neural Networks (CNNs)**, a transformative innovation in deep learning that has revolutionized computer vision tasks. In this notebook, we’ll explore the fundamental building blocks of CNNs and their applications in **image segmentation**, a task that involves classifying each pixel in an image into meaningful categories.\n",
    "\n",
    "CNNs are particularly well-suited for computer vision tasks due to their ability to learn spatial hierarchies of features, such as edges, textures, and object parts, by applying convolutional filters. These filters act as feature extractors, identifying patterns in local regions of an image.\n",
    "\n",
    "By the end of this notebook, you’ll understand how to:\n",
    "\n",
    "1) Visualize convolutional filters and grasp their role in feature extraction.\n",
    "2) Build and train a basic CNN model using PyTorch.\n",
    "3) Implement your first segmentation model and evaluate its performance on a dataset.\n",
    "\n",
    "Our journey begins with understanding the motivation behind CNNs and why they outperform traditional Multi-Layer Perceptrons (MLPs) for image analysis tasks. Next, we'll dive into the implementation of a segmentation pipeline, showcasing the practical power of CNNs.\n",
    "\n",
    "Let’s get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97380741",
   "metadata": {},
   "source": [
    "## Step 1: Understand the limitations of general MLPs\n",
    "In this section, we'll revisit the constraints inherent in general MLPs. We'll start by defining an MLP class and then explore the consequences when attempting to increase the number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a Multi-Layer Perceptron (MLP)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, hidden_layer_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers_list = []\n",
    "        layers_list.append(nn.Flatten())      \n",
    "\n",
    "        for i in range(num_hidden_layers):\n",
    "            if i == 0: # create first layers\n",
    "                layers_list.append(nn.Linear(32*32*1, hidden_layer_size)) # 32*32*3 = cifar image x,y size and RGB channels\n",
    "            else:\n",
    "                layers_list.append(nn.Linear(hidden_layer_size, hidden_layer_size))\n",
    "                \n",
    "            layers_list.append(nn.ReLU())\n",
    "\n",
    "        # Output layer\n",
    "        layers_list.append(nn.Linear(hidden_layer_size, 10))  # Assuming 10 classes for CIFAR-10\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layers_list = [1, 2, 5, 10]\n",
    "hidden_layer_size = 64\n",
    "\n",
    "\n",
    "# Loop through different numbers of hidden layers\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "    # Create and train the model\n",
    "    model = MLP(num_hidden_layers, hidden_layer_size)\n",
    "    \n",
    "    # Count the number of parameters\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Number of parameters for {num_hidden_layers}-layer MLP: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e539d4",
   "metadata": {},
   "source": [
    "You can clearly see that the number of **trainable parameters** increases heavily with an increase in hidden layers. Now let's train these different models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs, labels  # Assuming you're using GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy on the test set: {:.2f}%'.format(accuracy))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6db02b6",
   "metadata": {},
   "source": [
    "We will make use of the same dataset used in the first notebook, CIFAR-10. However to make the next part about convolutions a bit more intuitive we will transform the images to 1-channel grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1),# Converts to grayscale\n",
    "      \n",
    "])\n",
    "\n",
    "cifar_train = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "cifar_test = datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=10,\n",
    "                                          shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 10\n",
    "\n",
    "for num_hidden_layers in num_hidden_layers_list:\n",
    "    \n",
    "    # Create and train the model\n",
    "    model = MLP(num_hidden_layers, hidden_layer_size)\n",
    "    # Count the number of parameters\n",
    "    num_params = count_parameters(model)\n",
    "    print(f\"Number of parameters for {num_hidden_layers}-layer MLP: {num_params}\")\n",
    "    \n",
    "    # Start model training \n",
    "    start_time = time.time()\n",
    "    train_model(model, trainloader, nb_epochs, learning_rate)\n",
    "    end_time = time.time()\n",
    "       \n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"Time taken for {num_hidden_layers}-layer MLP: {execution_time:.2f} seconds\")\n",
    "    \n",
    "    # Test model\n",
    "    test_model(model, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc7fdf",
   "metadata": {},
   "source": [
    "## Questions\n",
    "**Q1**. What happens to the training time when you increase the number of hidden layers?\n",
    "\n",
    "**Q2**. How does the loss change with an increase in hidden layers?\n",
    "\n",
    "**Q3**. Why do you think the accuracy of the model decreases for the 10 layer MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae7bde",
   "metadata": {},
   "source": [
    "## Step 2: Implement Convolutional Filters\n",
    "First, let's observe how an image transforms after applying a convolutional filter. Experiment with defining at least three general convolutional filters and observe the resulting images after applying these filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c472e670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up a transform to convert images to tensors and grayscale\n",
    "transform = transforms.Compose([    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1), # Converts to grayscale\n",
    "      \n",
    "])\n",
    "\n",
    "# Load a single grayscale image\n",
    "cifar_train = datasets.CIFAR10(root='./data', train=True,\n",
    "                                download=True, transform=transform)\n",
    "\n",
    "image, label = cifar_train[4]\n",
    "gray_image = image.squeeze().numpy()\n",
    "\n",
    "# Function to apply 2D convolution with multiple filters\n",
    "def apply_convolutions(image, kernels):\n",
    "    image_tensor = torch.tensor(image).unsqueeze(0).unsqueeze(0).float()\n",
    "    results = [F.conv2d(image_tensor, kernel.unsqueeze(0).unsqueeze(0).float(), padding=0).squeeze().numpy() for kernel in kernels]\n",
    "    return results\n",
    "\n",
    "# Function to display images\n",
    "def display_images(images, titles):\n",
    "    plt.figure(figsize=(len(images) * 3, 3))\n",
    "    for i, (img, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(1, len(images), i)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Define multiple convolutional filters (kernels), these should be a 2x2 or 3x3 pytorch tensor\n",
    "kernels = [\n",
    "    torch.tensor([\n",
    "        [-1,  0,  1],\n",
    "        [-2,  0,  2],\n",
    "        [-1,  0,  1]\n",
    "    ], dtype=torch.float32),   # Sobel filter for horizontal edges\n",
    "    ...\n",
    "]\n",
    "\n",
    "\n",
    "# Apply 2D convolution with multiple filters\n",
    "results = apply_convolutions(gray_image, kernels)\n",
    "\n",
    "# Display original image and results\n",
    "display_images([gray_image] + results, ['Original Grayscale Image'] + ['Result after Convolution {}'.format(i+1) for i in range(len(results))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a31ae7",
   "metadata": {},
   "source": [
    "Now, attempt to implement a basic CNN architecture. Refer to the lecture slides for guidance on its structure and components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65377f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleCNN model, for visualization purpose please refer to the first convolutional layer as conv1.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # TODO: (1) make a CNN classification model consiting out of at least 3 convolutional building blocks. (2) the first convolutional\n",
    "        # layers should be refered to as self.conv1 for visualization purpose. (3) the output layer that returns the output \n",
    "        # for the 10 classes (NOTE: we flatten the images; see forward function)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70e8c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = SimpleCNN()\n",
    "nb_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Create and train the model\n",
    "num_params = count_parameters(simple_cnn)\n",
    "print(f\"Number of parameters for CNN: {num_params}\")\n",
    "\n",
    "# Start model training \n",
    "train_model(simple_cnn, trainloader, nb_epochs, learning_rate)\n",
    "test_model(simple_cnn, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cda81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_filters(model, layer_name, num_filters=16):\n",
    "    \"\"\"\n",
    "    Visualize filters from a specific convolutional layer in the model.\n",
    "\n",
    "    Parameters:\n",
    "        - model: The CNN model\n",
    "        - layer_name: The name of the convolutional layer (e.g., 'conv1', 'conv2', 'conv3')\n",
    "        - num_filters: Number of filters to visualize\n",
    "\n",
    "    Returns:\n",
    "        None (displays the visualizations)\n",
    "    \"\"\"\n",
    "    # Get the specified convolutional layer\n",
    "    layer = getattr(model, layer_name)\n",
    "\n",
    "    # Get the weights from the layer\n",
    "    filters = layer.weight.data.cpu().numpy()\n",
    "    \n",
    "    # Get min and max value of all filters to scale plots\n",
    "    min_value = filters.min()\n",
    "    max_value = filters.max()\n",
    "\n",
    "    # Plot the filters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in range(num_filters):\n",
    "        plt.subplot(2, num_filters // 2, i + 1)\n",
    "        plt.imshow(filters[i][0], cmap='gray', vmin=min_value, vmax=max_value)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Filter {i + 1}')\n",
    "\n",
    "    plt.suptitle(f'Filters from {layer_name}', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn.eval()\n",
    "\n",
    "# Visualize filters from the first convolutional layer\n",
    "visualize_filters(simple_cnn, 'conv1', num_filters=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30361631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters_to_image(model, image, layer_name):\n",
    "    \"\"\"\n",
    "    Apply individual filters from a specific convolutional layer to an input image.\n",
    "\n",
    "    Parameters:\n",
    "        - model: The CNN model\n",
    "        - image: Input image (NumPy array)\n",
    "        - layer_name: The name of the convolutional layer (e.g., 'conv1', 'conv2', 'conv3')\n",
    "\n",
    "    Returns:\n",
    "        None (displays the visualizations)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the specified convolutional layer\n",
    "    layer = getattr(model, layer_name)\n",
    "\n",
    "    # Get the individual filters\n",
    "    filters = layer.weight.data\n",
    "\n",
    "    # Apply and visualize each filter on the input image\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(filters.shape[0]):\n",
    "        output = F.relu(F.conv2d(image, filters[i].unsqueeze(0).float(), padding=1))\n",
    "        plt.subplot(2, filters.shape[0] // 2, i + 1)\n",
    "        plt.imshow(output.squeeze().detach().numpy(), cmap='gray')\n",
    "        plt.title(f'Filter {i + 1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.suptitle(f'Application of Filters from {layer_name} to Input Image', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Load a single grayscale image\n",
    "cifar_train = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "sample_image, _ = cifar_train[4]\n",
    "\n",
    "# Apply and visualize filters\n",
    "apply_filters_to_image(simple_cnn, sample_image, 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8a05d",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Q4**. How does the basic CNN model performance compare against a MLP based model with comparable number of trainable parameters?\n",
    "\n",
    "**Q5**. What happens to the loss from the CNN model compared to the MLP based models?\n",
    "\n",
    "**Q6**. What can you tell about the learned filters in the first convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eeb13a",
   "metadata": {},
   "source": [
    "# Step 3: Implement a segmentation model\n",
    "Let's transition from the CIFAR-10 classification dataset to the VOC2012 segmentation dataset. In this section of the notebook, our attention shifts towards semantic segmentation, where we'll explore techniques for delineating objects within images. Due to computational constraints, we'll be training our model on a CPU basis, necessitating a resize of the images to 32x32 pixels. While this resolution may limit the quality of segmentation results, this exercise serves as a valuable opportunity to grasp the fundamentals of building and enhancing a segmentation model. If you have access to a GPU your own your ofcourse free to try and train a segmentation model on a higher resolution with more epochs and get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation for input images and masks\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "target_transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32),transforms.InterpolationMode.NEAREST),  # Resize masks\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Download and load PASCAL VOC dataset\n",
    "train_dataset = datasets.VOCSegmentation(root='./data', year='2012', image_set='train', download=True, transform=transform, target_transform=target_transforms)\n",
    "val_dataset = datasets.VOCSegmentation(root='./data', year='2012', image_set='val', download=True, transform=transform, target_transform=target_transforms)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "trainloader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=8, pin_memory=True)\n",
    "validationloader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f976e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Segmentation model\n",
    "class SegmentationCNN(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(SegmentationCNN, self).__init__()\n",
    "        # TODO: Implement a convolutional neural network for image segmentation.\n",
    "        # (1) Design the layers to extract features from input images.\n",
    "        # (2) Ensure the final layer outputs a tensor of shape [B, num_classes, X, Y],\n",
    "        #     where B is the batch size, and X and Y are the dimensions of the output segmentation map.\n",
    "        #     This indicates the segmentation prediction for each pixel in the input image.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622207e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_segmentation(model, train_loader, num_epochs=5, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, masks in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            masks = (masks * 255)\n",
    "\n",
    "            loss = criterion(outputs, masks.long().squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ba7a",
   "metadata": {},
   "source": [
    "Unfortunately, this training process takes slightly longer compared to earlier training loops (+- 30 min for 30 epochs). However, it's worthwhile to take some time to consider potential optimizations that could enhance its speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58484529",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 30\n",
    "learning_rate = 0.0001\n",
    "\n",
    "segmentation_model = SegmentationCNN(in_channels=3, num_classes=21)\n",
    "\n",
    "train_model_segmentation(segmentation_model, trainloader, nb_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ef1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize examples (No modifications needed)\n",
    "colors = {\n",
    "        0: (0, 0, 0),\n",
    "        1: (31, 119, 180),\n",
    "        2: (255, 127, 14),  \n",
    "        3: (44, 160, 44),   \n",
    "        4: (214, 39, 40), \n",
    "        5: (148, 103, 189),\n",
    "        6: (140, 86, 75),\n",
    "        7: (227, 119, 194),\n",
    "        8: (127, 127, 127),\n",
    "        9: (188, 189, 34),\n",
    "        10: (23, 190, 207),\n",
    "        11: (174, 199, 232),\n",
    "        12: (255, 187, 120),\n",
    "        13: (152, 223, 138),\n",
    "        14: (255, 152, 150),\n",
    "        15: (197, 176, 213),\n",
    "        16: (196, 156, 148),\n",
    "        17: (247, 182, 210),\n",
    "        18: (199, 199, 199),\n",
    "        19: (219, 219, 141),\n",
    "        255: (0, 0, 0)\n",
    "}\n",
    "\n",
    "class_names = [\n",
    "    'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n",
    "    'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "    'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "\n",
    "def mask_to_rgb(mask, class_to_color):\n",
    "    \"\"\"\n",
    "    Converts a numpy mask with multiple classes indicated by integers to a color RGB mask.\n",
    "\n",
    "    Parameters:\n",
    "        mask (numpy.ndarray): The input mask where each integer represents a class.\n",
    "        class_to_color (dict): A dictionary mapping class integers to RGB color tuples.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: RGB mask where each pixel is represented as an RGB tuple.\n",
    "    \"\"\"\n",
    "    # Get dimensions of the input mask\n",
    "    height, width = mask.shape\n",
    "\n",
    "    # Initialize an empty RGB mask\n",
    "    rgb_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    # Iterate over each class and assign corresponding RGB color\n",
    "    for class_idx, color in class_to_color.items():\n",
    "        # Mask pixels belonging to the current class\n",
    "        class_pixels = mask == class_idx\n",
    "        # Assign RGB color to the corresponding pixels\n",
    "        rgb_mask[class_pixels] = color\n",
    "\n",
    "    return rgb_mask\n",
    "\n",
    "def visualize_segmentation(model, dataloader, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualizes segmentation results from a given model using a dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The segmentation model to visualize.\n",
    "        dataloader (torch.utils.data.DataLoader): Dataloader providing image-mask pairs.\n",
    "        num_examples (int, optional): Number of examples to visualize. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, masks) in enumerate(dataloader):\n",
    "            if i >= num_examples:\n",
    "                break\n",
    "            \n",
    "            outputs = model(images)\n",
    "            outputs = torch.softmax(outputs, dim=1)\n",
    "            predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "            images = images.numpy()\n",
    "            masks = masks.numpy()*255\n",
    "\n",
    "            predicted = predicted.numpy()\n",
    "\n",
    "            for j in range(images.shape[0]):\n",
    "                image = renormalize_image(images[j].transpose(1, 2, 0))\n",
    "\n",
    "                mask = masks[j].squeeze()\n",
    "                pred_mask = predicted[j]\n",
    "                                \n",
    "                # Convert mask and predicted mask to RGB for visualization\n",
    "                mask_rgb = mask_to_rgb(mask, colors)\n",
    "                pred_mask_rgb = mask_to_rgb(pred_mask, colors)\n",
    "                \n",
    "                # Get unique classes present in the ground truth and predicted masks\n",
    "                unique_classes_gt = np.unique(mask)\n",
    "                unique_classes_pred = np.unique(pred_mask)\n",
    "                \n",
    "                unique_classes_gt = np.delete(unique_classes_gt, [0, -1])\n",
    "                unique_classes_pred= np.delete(unique_classes_pred, 0)\n",
    "                \n",
    "                unique_classes_gt[unique_classes_gt == 255] = 0\n",
    "                unique_classes_pred[unique_classes_pred == 255] = 0\n",
    "                \n",
    "                \n",
    "                # Map class indices to class names from the VOC2012 dataset\n",
    "                classes_gt = [class_names[int(idx)] for idx in unique_classes_gt]\n",
    "                classes_pred = [class_names[int(idx)] for idx in unique_classes_pred]\n",
    "                \n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title('Image')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(mask_rgb)\n",
    "                plt.title(f'Ground Truth Mask Classes:\\n {classes_gt}')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(pred_mask_rgb)\n",
    "                plt.title(f'Predicted Mask Predicted Classes:\\n {classes_pred}')\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.show()\n",
    "                \n",
    "\n",
    "def renormalize_image(image):\n",
    "    \"\"\"\n",
    "    Renormalizes the image to its original range.\n",
    "    \n",
    "    Args:\n",
    "        image (numpy.ndarray): Image tensor to renormalize.\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Renormalized image tensor.\n",
    "    \"\"\"\n",
    "    mean = [0.5, 0.5, 0.5]\n",
    "    std = [0.5, 0.5, 0.5]  \n",
    "    renormalized_image = image * std + mean\n",
    "    return renormalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91dc5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Think about results\n",
    "visualize_segmentation(segmentation_model, validationloader, num_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f284a96",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "**Q7**. Which specific modifications or enhancements were implemented to improve the performance of the model?\n",
    "\n",
    "**Q8**. What are the primary challenges or obstacles encountered by the model when addressing the segmentation problem, and what possible solution can we use to tackle these challenges? Think about the classes the model already segmentens correctly and which classes are misted completely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
