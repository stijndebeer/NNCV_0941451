{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8: Score-based Models (Score Matching and Flow Matching)\n",
    "\n",
    "Extending the diffusion concept to infinitely many steps introduces the challenge of handling continuous time, which requires the use of differential equations. Score-based models provide an elegant alternative to diffusion models by focusing directly on estimating the gradients of data distributions—referred to as scores. These gradients enable a continuous transformation from noise to data, forming the foundation for building continuous-time generative models. This approach not only simplifies the framework but also offers a unique mechanism for transforming noise into data.\n",
    "\n",
    "For this task, we will build and train a **Score-based Model**:\n",
    "\n",
    "- **Score-based Model**: A type of generative model that learns to generate data by estimating the score (gradient of the data distribution) and using it to transform noise into data.\n",
    "  - **Score Matching**: A technique to train the model by matching the score of the data distribution.\n",
    "  - **Flow Matching**: A technique to train the model by matching the flow of the data distribution.\n",
    "\n",
    "One key tool utilized in Score-based Models is **Stochastic Gradient Langevin Dynamics (SGLD)** [1]. SGLD is a method for sampling from the real data distribution, $p_{\\text{real}}(x)$, by iteratively refining a randomly initialized point in the data space. Starting with $\\hat{x}$, where $p_{\\text{real}}(\\hat{x}) \\approx 0$, the process updates this point to move it toward regions of higher probability. The update rule is given by:\n",
    "\n",
    "$$\n",
    "x_{t+\\Delta t} = x_t + \\alpha \\nabla_{x_t} \\ln p_{\\text{real}}(x_t) + \\eta \\epsilon,\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_t$: Current sample at time $t$,\n",
    "- $\\nabla_{x_t} \\ln p_{\\text{real}}(x_t)$: Gradient of the log-probability at $x_t$, guiding the sample toward high-probability regions,\n",
    "- $\\alpha > 0$: Step size controlling the gradient update magnitude,\n",
    "- $\\eta > 0$: Scaling factor for the noise term,\n",
    "- $\\epsilon \\sim \\mathcal{N}(0, I)$: Gaussian noise ensuring exploration of the data space.\n",
    "\n",
    "Through this iterative process, SGLD generates samples that align with $p_{\\text{real}}(x)$, effectively transforming noise into realistic data.\n",
    "\n",
    "By the end of this notebook, you’ll understand how to:\n",
    "1. Build and train a score-based model using PyTorch.\n",
    "2. Sample from a Score Matching Model.\n",
    "3. sample from a Flow Matching Model.\n",
    "\n",
    "Let’s dive in and start building!\n",
    "\n",
    "---\n",
    "\n",
    "#### References\n",
    "<div style=\"font-size: smaller;\">  \n",
    "[1] Welling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 681-688).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Create a Dataset of 2D Points\n",
    "\n",
    "In this step, we will import the necessary libraries and create a dataset of 2D points that form a smiley face. This dataset will be used to train our Denoising Diffusion Model. Using a simpler dataset reduces the computational requirements of this notebook, but allow for a better visual representation of how these models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code based on https://bm371613.github.io/conditional-flow-matching/ ###\n",
    "\n",
    "def create_dataset(size: int = 100000):\n",
    "    '''\n",
    "    Create a dataset of size `size` of 2D points that have a shape of a smiley face.\n",
    "    Args:\n",
    "        size: int: The number of points to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated dataset.\n",
    "    '''\n",
    "    complex_points = torch.polar(torch.tensor(1.0), torch.rand(size) * 2 * torch.pi)\n",
    "    X = torch.stack((complex_points.real, complex_points.imag)).T\n",
    "    upper = complex_points.imag > 0\n",
    "    left = complex_points.real < 0\n",
    "    X[upper, 1] = 0.5\n",
    "    X[upper & left, 0] = -0.5\n",
    "    X[upper & ~left, 0] = 0.5\n",
    "    noise = torch.zeros_like(X)\n",
    "    noise[upper] = torch.randn_like(noise[upper]) * 0.10\n",
    "    noise[~upper] = torch.randn_like(noise[~upper]) * 0.05\n",
    "    X += noise\n",
    "    X -= X.mean(axis=0)\n",
    "    X /= X.std(axis=0)\n",
    "    return X + noise\n",
    "\n",
    "def plot_dataset(X, bins, ax=None, verbose=True, **kwargs):\n",
    "    '''\n",
    "    Plot a 2D dataset.\n",
    "    Args:\n",
    "        X: torch.Tensor: The dataset to plot.\n",
    "        bins: int: The number of bins to use for the histogram.\n",
    "        ax: plt.Axes: The axes to plot on. If None, the current axes will be used.\n",
    "        verbose: bool: Whether to show the axis labels.\n",
    "        **kwargs: dict: Additional keyword arguments to pass to `ax.set`.\n",
    "    '''\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.hist2d(*X.T, bins=bins, cmap='gray')\n",
    "    if verbose:\n",
    "        ax.set_xlabel('feature 0')\n",
    "        ax.set_ylabel('feature 1')\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    ax.set(**kwargs)\n",
    "\n",
    "# Create the smiley face dataset and plot it.\n",
    "dataset = create_dataset()\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plot_dataset(dataset, bins=64, title='Smiley face dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define a suitable Score-based Model\n",
    "\n",
    "1. The main requirement for a model capable of parametrizing these tasks is **Output Dimensionality**:\n",
    "    - **The model's output must match the input dimensionality**. Each score corresponds directly to each pixel (or input feature), whether it's predicting gradients, noise values, or a vector field.\n",
    "\n",
    "2. The second requirement is also straightforward an simple to ensure - **the model must take the time step as a conditioning input**.\n",
    "    - The time step provides information about the diffusion/matching process stage, enabling the model to predict the appropriate scores or noise values.\n",
    "\n",
    "3. This means that the **Model Architecture is flexible**. While U-Nets are commonly used for high-dimensional inputs like images, you can use any architecture that supports the task. Recent approaches even use transformers.\n",
    "    - For this notebook, since the distribution we are modelling is not complex, we will use a **Multilayer Perceptron (MLP)** for faster training and inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroToOneTimeEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        '''\n",
    "        Create a time embedding that maps time to a 2D embedding.\n",
    "        Args:\n",
    "            dim: int: The dimensionality of the embedding\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.register_buffer('freqs', torch.arange(1, dim // 2 + 1) * torch.pi)\n",
    "\n",
    "    def forward(self, t):\n",
    "        '''\n",
    "        Forward pass of the time embedding.\n",
    "        Args:\n",
    "            t: torch.Tensor: The time to embed.\n",
    "        Returns:\n",
    "            torch.Tensor: The embedded time.\n",
    "        '''\n",
    "        emb = self.freqs * t[..., None]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_features, time_embedding_size=8, n_blocks=5):\n",
    "        '''\n",
    "        Create an MLP with residual connections.\n",
    "        Args:\n",
    "            n_features: int: The number of input and output features.\n",
    "            time_embedding_size: int: The size of the time embedding to use.\n",
    "            n_blocks: int: The number of residual blocks to use.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.time_embedding = ZeroToOneTimeEmbedding(time_embedding_size)\n",
    "        hidden_size = n_features + time_embedding_size\n",
    "        blocks = []\n",
    "        for _ in range(n_blocks):\n",
    "            blocks.append(nn.Sequential(\n",
    "                None, # A batch norm layer would go here, taking an input size of `hidden_size`.\n",
    "                None, # A linear layer with hidden_size inputs and outputs.\n",
    "                None, # A ReLU activation.\n",
    "            ))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.final = None # A linear layer with hidden_size inputs and that outputs n_features.\n",
    "\n",
    "\n",
    "    def forward(self, X, time):\n",
    "        '''\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "            X: torch.Tensor: The input features.\n",
    "            time: torch.Tensor: The time to embed.\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the model.\n",
    "        '''\n",
    "        X = torch.cat([X, self.time_embedding(time)], axis=1)\n",
    "        for block in self.blocks:\n",
    "            X = X + block(X)\n",
    "        X = self.final(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Matching\n",
    "\n",
    "One major challenge in applying SGLD is that the real data distribution, $ p_{\\text{real}}(x) $, is unknown. While we could turn to generative models like GANs, VAEs, or normalizing flows to approximate $ p_{\\text{real}}(x) $, there is an alternative approach. Upon closer examination of the SGLD update equation, we realize that $ p_{\\text{real}}(x) $ itself is not necessary—what we truly need is the gradient of its logarithm, $ \\nabla_x \\ln p_{\\text{real}}(x) $, known as the score function. The score function provides a vector field pointing toward the modes of the real data distribution. For example, in the case of a multimodal distribution, the score function can be visualized as vectors on a grid directing Langevin dynamics toward high-probability regions.\n",
    "\n",
    "This insight allows reformulation: instead of modeling $ p_{\\text{real}}(x) $, we focus on learning the score function. As a gradient, the score function shares the shape of $ x $ and can be represented as a vector. By learning $ s_\\theta(x) $, we enable SGLD to produce samples that follow $ p_{\\text{real}}(x) $ without explicitly modeling it. To achieve this, we optimize the difference between the true score function and a parameterized approximation $ s_\\theta(x) $:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\frac{1}{2} \\int \\|s_\\theta(x) - \\nabla_x \\ln p_{\\text{real}}(x)\\|^2 p_{\\text{real}}(x) \\, dx.\n",
    "$$\n",
    "\n",
    "Directly using this objective is challenging because $ p_{\\text{data}}(x) $ is a discrete distribution represented as a sum of Dirac delta functions, which is non-differentiable. To address this, small Gaussian noise with variance $ \\sigma^2 $ is added to the data. Perturbed data is represented as $ \\tilde{x}_n = x_n + \\sigma \\cdot \\epsilon $, where $ \\epsilon \\sim \\mathcal{N}(0, I) $. This transformation smooths $ p_{\\text{data}}(x_n) $ into a Gaussian mixture. This smoothed distribution makes the optimization problem differentiable. Using the closed-form score function for a Gaussian distribution, the gradient of the log-likelihood with respect to $ \\tilde{x}_n $ is:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\tilde{x}} \\ln \\mathcal{N}(\\tilde{x}_n | x_n, \\sigma^2) = -\\frac{1}{\\sigma} \\epsilon.\n",
    "$$\n",
    "\n",
    "Applying this result to the objective, we get the differentiable objective:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{E}_{N(\\epsilon | 0, I)} \\left[ \\frac{1}{2\\sigma^2} \\left\\| \\epsilon - \\tilde{s}_\\theta(\\tilde{x}) \\right\\|^2 \\right],\n",
    "$$\n",
    "\n",
    "where $ \\tilde{s}_\\theta(x) = -\\sigma s_\\theta(\\tilde{x}) $. Learning $ \\tilde{s}_\\theta(\\tilde{x}) $ by optimizing this objective is referred to as score matching.\n",
    "\n",
    "To extend this formulation to a time-dependent framework, we can modify the equation for $ \\sigma_t $ to allow for time-dependent values of $ \\sigma $ [2]. Specifically, we can express the time-dependent version of $ \\sigma_t $ as:\n",
    "\n",
    "$$\n",
    "\\sigma_t = \\sqrt{\\frac{\\sigma_k^{2t} - 1}{2 \\log(\\sigma_k)}}\n",
    "$$\n",
    "\n",
    "In this version, $ \\sigma_k $ is a constant.\n",
    "\n",
    "---\n",
    "\n",
    "#### References\n",
    "<div style=\"font-size: smaller;\">  \n",
    "[2] Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the Score Matching Model\n",
    "\n",
    "1. **repeat**  \n",
    "2. &nbsp;&nbsp;&nbsp;Sample $x \\sim q(x)$  \n",
    "3. &nbsp;&nbsp;&nbsp;Sample $t \\sim \\text{Uniform}(\\{0, \\dots, 1\\})$  \n",
    "4. &nbsp;&nbsp;&nbsp;Sample $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$  \n",
    "5. &nbsp;&nbsp;&nbsp;Calculate the noisy version of the data: $ \\tilde{x_t} = x + \\sigma_t \\cdot \\epsilon $.\n",
    "6. &nbsp;&nbsp;&nbsp;Calculate the score $ \\tilde{s}_\\theta(\\tilde{x}) $. \n",
    "7. &nbsp;&nbsp;&nbsp;Take a gradient descent step on $\\nabla_\\theta \\frac{1}{2\\sigma^2} \\| \\epsilon - \\tilde{s}_\\theta(\\tilde{x}) \\|^2$  \n",
    "8. **until converged**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_t(sigma_k, t):\n",
    "    \"\"\"Compute the sigma for a certain t.\n",
    "\n",
    "    Args: \n",
    "        sigma_k: The $\\sigma_k$ in our SDE.    \n",
    "        t: A vector of time steps. \n",
    "\n",
    "    Returns:\n",
    "        The ssigma_t.\n",
    "    \"\"\"\n",
    "    return None # Implement this function to return the sigma_t based on the formula in the text.\n",
    "\n",
    "def score_matching_loss(model, x, sigma_k=25):\n",
    "    \"\"\"The loss function for training score-based generative models.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model instance that represents a \n",
    "        time-dependent score-based model.\n",
    "        x: A mini-batch of training data. \n",
    "        sigma_k: The $\\sigma_k$ in our SDE.   \n",
    "    \"\"\"\n",
    "    t = torch.rand(size=(x.shape[0],), device=x.device)  \n",
    "    z = torch.randn_like(x)\n",
    "    sigma = sigma_t(sigma_k, t)\n",
    "    perturbed_x = x + z * sigma[:, None]\n",
    "    score = model(perturbed_x, t)\n",
    "    return torch.mean(torch.sum((score * sigma[:, None] + z)**2, dim=(1)))\n",
    "\n",
    "def training_loop(model, sigma_k, n_steps, batch_size, lr):\n",
    "    '''\n",
    "    Train a model.\n",
    "    Args:\n",
    "        model: nn.Module: The model to train.\n",
    "        sigma_k: float: The $\\sigma_k$ parameter to use.\n",
    "        n_steps: int: The number of training steps to take.\n",
    "        batch_size: int: The number of samples in each batch.\n",
    "        lr: float: The learning rate to use.\n",
    "    '''\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = 'cpu'\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    step_bar = trange(n_steps, desc='Steps')\n",
    "    for step in step_bar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = create_dataset(size=batch_size)\n",
    "        loss = score_matching_loss(model, batch, sigma_k=sigma_k)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            step_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "model = MLP(n_features=2)\n",
    "training_loop(model, sigma_k=10, n_steps=6000, batch_size=10000, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Sample from the Score Matching Model\n",
    "\n",
    "1. **Sample an initial point** $ x_1 \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{t=1}^2\\mathbf{I}) $.\n",
    "2. **Run Langevin dynamics** for $ T $ steps, where $ \\Delta = \\frac{1}{T} $, and $ \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}) $ is Gaussian noise:\n",
    "   $$ \\alpha_t = \\Delta \\cdot(\\sigma_k^t)^2$$\n",
    "   $$\\eta_t = \\sqrt{\\alpha_t} $$\n",
    "   $$ x_{t+\\Delta} = x_t + \\alpha_t \\tilde{s}_\\theta(x_t, t) + \\eta_t \\cdot \\epsilon,$$\n",
    "   where $ \\tilde{s}_\\theta(x_t, t) $ is the score function at time $ t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_coeff(sigma, t):\n",
    "        \"\"\"Compute the diffusion coefficient of our SDE.\n",
    "\n",
    "        Args:\n",
    "            t: A vector of time steps.\n",
    "            sigma: The $\\sigma$ in our SDE.\n",
    "\n",
    "        Returns:\n",
    "            The vector of diffusion coefficients.\n",
    "        \"\"\"\n",
    "        return (sigma**t)\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_score_matching(model, steps=100, sigma_k=10, num_samples=10000):\n",
    "    '''\n",
    "    Sample from the model using the score matching method.\n",
    "    Args:\n",
    "        model: nn.Module: The model to sample from.\n",
    "        steps: int: The number of steps to take.\n",
    "        sigma_k: float: The $\\sigma_k$ parameter to use.\n",
    "    '''\n",
    "    model.eval()\n",
    "    t = torch.linspace(1, 0, steps+1)\n",
    "    x = torch.randn(num_samples, 2)*sigma_t(sigma_k, torch.full((num_samples,), t[0]))[:, None]\n",
    "    step_size = 1. / steps\n",
    "    for i in tqdm(range(steps)):\n",
    "        alpha = None # Implement this line to compute the alpha value, use the diffusion_coeff function.\n",
    "        score = model(x, torch.full((num_samples,), t[i]))\n",
    "        x = None # Implement this line to update the value of x, according to the formula in the text.\n",
    "    return x\n",
    "\n",
    "# Sample from the model and plot the results.\n",
    "samples = sample_score_matching(model, steps=100, sigma_k=10)\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plot_dataset(samples, bins=64, title='Samples from the score matching model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Matching\n",
    "\n",
    "Flow Matching [3] is introduced by considering the following ordinary differential equation (ODE):\n",
    "\n",
    "$$\n",
    "\\frac{dx_t}{dt} = v(x_t, t),\n",
    "$$\n",
    "\n",
    "where $ v(x_t, t) $ represents the vector field that defines the dynamics of the system. The vector field governs how the data evolves over time, and by parameterizing it with a neural network $ v_\\theta(x_t, t) $, the model becomes a Neural ODE [4]. To remain consistent with the Flow Matching literature, time progresses from noise ($ t = 0 $) to data ($ t = 1 $), defining the forward dynamics and the generative process. This differs from diffusion models (and score matching models), where the time direction is reversed, evolving from data to noise. Furthermore, it is assumed that the initial distribution $ q_0(x) $, such as a standard Gaussian, is known, and the target data distribution $ q_1(x) $ is the distribution that needs to be matched. The goal is to learn a trajectory from the noise distribution $ q_0(x) $ to the data distribution $ q_1(x) $. The output is obtained by solving the ODE, i.e., integrating over time $ t $, which transforms the noise into data:\n",
    "\n",
    "$$\n",
    "x_1 = \\int_0^1 v_\\theta(x_t, t) \\, dt. \n",
    "$$\n",
    "\n",
    "However, the vector field $ v(x_t, t) $ and the distributions $ p_t(x) $ are not known. To address this issue, a modified approach can be introduced by introducing an additional variable $ z $, which is sampled from a known distribution $ q(z) $. The conditional ODE then takes the form:\n",
    "\n",
    "$$\n",
    "\\frac{dx_t}{dt} = v(x_t, t; z),\n",
    "$$\n",
    "\n",
    "where $ z $ can represent additional information, such as the data $ x_1 $. The conditional flow matching loss is then formulated as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{CFM}(\\theta) = \\mathbb{E}_{t \\sim U(0, 1), x_t \\sim p_t(x|z), z \\sim q(z)} \\left[ \\|v_\\theta(x_t, t) - v(x_t, t; z)\\|^2 \\right].\n",
    "$$\n",
    "\n",
    "In this setup, the objective is to minimize the difference between the model's vector field $ v_\\theta(x_t, t) $ and the true conditional vector field $ v(x_t, t; z) $, where $ x_t $ is sampled from the distribution $ p_t(x|z) $ conditioned on $ z $, and $ z $ is sampled from the known distribution $ q(z) $. A natural choice for a conditional probability paths is to use Optimal Transport [5], i.e., simply linearly interpolate between $ x_0 \\sim q(x_0) $ and $ x_1 \\sim q(x_1) $, with:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_t = t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_0.\n",
    "$$\n",
    "\n",
    "Based on this proposed optimal transport, we know:\n",
    "\n",
    "$$\n",
    "v(\\mathbf{x}_t, t, \\mathbf{x}_1) = \\mathbf{x}_1 - \\mathbf{x}_0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### References\n",
    "<div style=\"font-size: smaller;\"> \n",
    "[3] Lipman, Y., Havasi, M., Holderrieth, P., Shaul, N., Le, M., Karrer, B., ... & Gat, I. (2024). Flow Matching Guide and Code. arXiv preprint arXiv:2412.06264. <br>\n",
    "[4] Chen, R. T., Rubanova, Y., Bettencourt, J., & Duvenaud, D. K. (2018). Neural ordinary differential equations. Advances in neural information processing systems, 31.<br>\n",
    "[5] Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., & Le, M. (2022). Flow matching for generative modeling. arXiv preprint arXiv:2210.02747.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train the Flow Matching Model\n",
    "\n",
    "1. **repeat**   \n",
    "2. &nbsp;&nbsp;&nbsp;Sample $ t \\sim U(0, 1) $.\n",
    "3. &nbsp;&nbsp;&nbsp;Sample from the training data distribution $ x_1 \\sim q(x_1) $.\n",
    "4. &nbsp;&nbsp;&nbsp;Sample from the prior distribution $ x_0 \\sim q(x_0) $ (or $ x_0 \\sim \\mathcal{N}(0, \\mathbf{I}) $).\n",
    "5. &nbsp;&nbsp;&nbsp;Determine $ x_t = t x_1 + (1 - t) x_0 $.\n",
    "6. &nbsp;&nbsp;&nbsp;Calculate the loss $\\ell_{\\text{CFM}} = \\|v_\\theta(x_t, t) - v(x_t, t, x_1)\\|^2.$\n",
    "7. &nbsp;&nbsp;&nbsp;Update parameters $ \\theta $.\n",
    "8. **until converged**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_matching_loss(model, x):\n",
    "    \"\"\"The loss function for training flow matching generative models.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model instance that represents a \n",
    "        time-dependent score-based model.\n",
    "        x: A mini-batch of training data.    \n",
    "    \"\"\"\n",
    "    loss_fn = nn.MSELoss()\n",
    "    t = torch.rand(size=(x.shape[0],), device=x.device)  \n",
    "    z = torch.randn_like(x)\n",
    "    x_t = None # Implement this line to compute the x_t value according to step 5 of the algorithm.\n",
    "    predicted_flow = model(x_t, t)\n",
    "    optimal_flow = None # Implement this line to compute the optimal flow.\n",
    "    return loss_fn(predicted_flow, optimal_flow)\n",
    "\n",
    "def training_loop(model, n_steps, batch_size, lr):\n",
    "    '''\n",
    "    Train a model.\n",
    "    Args:\n",
    "        model: nn.Module: The model to train.\n",
    "        n_steps: int: The number of training steps to take.\n",
    "        batch_size: int: The number of samples in each batch.\n",
    "        lr: float: The learning rate to use.\n",
    "    '''\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    device = 'cpu'\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    step_bar = trange(n_steps, desc='Steps')\n",
    "    for step in step_bar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = create_dataset(size=batch_size)\n",
    "        loss = flow_matching_loss(model, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            step_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "model = MLP(n_features=2)\n",
    "training_loop(model, n_steps=2000, batch_size=10000, lr=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sample from the Flow Matching Model\n",
    "\n",
    "1. Sample from the prior distribution $ x_0 \\sim \\mathcal{N}(0, \\mathbf{I}) $.\n",
    "2. Run the forward Euler method from $ t = 0 $ until $ t = 1 $ with step size $ \\Delta $:\n",
    "   $$\n",
    "   x_{t+\\Delta} = x_t + v_\\theta(x_t, t) \\Delta\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_flow_matching(model, steps=100, num_samples=10000):\n",
    "    '''\n",
    "    Sample from the model using the flow matching method.\n",
    "    Args:\n",
    "        model: nn.Module: The model to sample from.\n",
    "        steps: int: The number of steps to take.\n",
    "    '''\n",
    "    model.eval()\n",
    "    t = torch.linspace(0, 1, steps+1)\n",
    "    x = torch.randn(num_samples, 2)\n",
    "    step_size = 1. / steps\n",
    "    for i in tqdm(range(steps)):\n",
    "        x = None # Implement this line to update the value of x, according to the formula in the text.\n",
    "    return x\n",
    "\n",
    "# Sample from the model and plot the results.\n",
    "samples = sample_flow_matching(model, steps=100)\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plot_dataset(samples, bins=64, title='Samples from the flow matching model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
