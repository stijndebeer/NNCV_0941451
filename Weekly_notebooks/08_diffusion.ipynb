{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Denoising Diffusion Models (DDPMs and DDIMs)\n",
    "\n",
    "Welcome to this Week 8 hands-on tutorial! In this notebook, we’ll explore how to train Denoising Diffusion Probabilistic Models (DDPMs) using **PyTorch**. These models are powerful tools for generating high-quality data samples by iteratively denoising a noisy input.\n",
    "\n",
    "For this task, we will build and train a **Denoising Diffusion Model. Diffusion models[1,2] can be understood as a type of Hierarchical Variational Autoencoder. The framework consists of two trajectories:  \n",
    "\n",
    "1. **Bottom-Up Trajectory (Variational Posterior)**  \n",
    "   - Characterized by a diffusion process, such as Gaussian diffusion.  \n",
    "   - This path introduces Gaussian noise at each layer, eventually transforming the latent variable into a standard Gaussian distribution.  \n",
    "   - This trajectory can be fixed and does not require learnable parameters.\n",
    "\n",
    "2. **Top-Down Trajectory**  \n",
    "   - Parameterized by deep neural networks to represent the reversed diffusion process.  \n",
    "\n",
    "The explicit design of the bottom-up path ensures convergence to a standard Gaussian in the last layer, inherently avoiding the issue of posterior collapse.\n",
    "\n",
    "To accelerate the sampling process, we will use both the original DDPM sampler and the **Denoising Diffusion Implicit Models (DDIM)** sampler. The DDIM sampler provides a more efficient way to generate samples by reducing the number of denoising steps required.\n",
    "\n",
    "By the end of this notebook, you’ll understand how to:\n",
    "1. Build and train a DDPM using PyTorch.\n",
    "2. Implement and use the original DDPM sampler to generate data samples.\n",
    "3. Implement and use the DDIM sampler to accelerate the sampling process.\n",
    "\n",
    "Let’s dive in and start building!\n",
    "\n",
    "---\n",
    "\n",
    "#### References\n",
    "<div style=\"font-size: smaller;\">  \n",
    "[1] Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015, June). Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning (pp. 2256-2265). PMLR.\n",
    "\n",
    "[2] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in neural information processing systems, 33, 6840-6851.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries and Create a Dataset of 2D Points\n",
    "\n",
    "In this step, we will import the necessary libraries and create a dataset of 2D points that form a smiley face. This dataset will be used to train our Denoising Diffusion Model. Using a simpler dataset reduces the computational requirements of this notebook, but allow for a better visual representation of how these models work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code based on https://bm371613.github.io/conditional-flow-matching/ ###\n",
    "\n",
    "def create_dataset(size: int = 100000):\n",
    "    '''\n",
    "    Create a dataset of size `size` of 2D points that have a shape of a smiley face.\n",
    "    Args:\n",
    "        size: int: The number of points to generate.\n",
    "    Returns:\n",
    "        torch.Tensor: The generated dataset.\n",
    "    '''\n",
    "    complex_points = torch.polar(torch.tensor(1.0), torch.rand(size) * 2 * torch.pi)\n",
    "    X = torch.stack((complex_points.real, complex_points.imag)).T\n",
    "    upper = complex_points.imag > 0\n",
    "    left = complex_points.real < 0\n",
    "    X[upper, 1] = 0.5\n",
    "    X[upper & left, 0] = -0.5\n",
    "    X[upper & ~left, 0] = 0.5\n",
    "    noise = torch.zeros_like(X)\n",
    "    noise[upper] = torch.randn_like(noise[upper]) * 0.10\n",
    "    noise[~upper] = torch.randn_like(noise[~upper]) * 0.05\n",
    "    X += noise\n",
    "    X -= X.mean(axis=0)\n",
    "    X /= X.std(axis=0)\n",
    "    return X + noise\n",
    "\n",
    "def plot_dataset(X, bins, ax=None, verbose=True, **kwargs):\n",
    "    '''\n",
    "    Plot a 2D dataset.\n",
    "    Args:\n",
    "        X: torch.Tensor: The dataset to plot.\n",
    "        bins: int: The number of bins to use for the histogram.\n",
    "        ax: plt.Axes: The axes to plot on. If None, the current axes will be used.\n",
    "        verbose: bool: Whether to show the axis labels.\n",
    "        **kwargs: dict: Additional keyword arguments to pass to `ax.set`.\n",
    "    '''\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.hist2d(*X.T, bins=bins, cmap='gray')\n",
    "    if verbose:\n",
    "        ax.set_xlabel('feature 0')\n",
    "        ax.set_ylabel('feature 1')\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    ax.set(**kwargs)\n",
    "\n",
    "# Create the smiley face dataset and plot it.\n",
    "dataset = create_dataset()\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plot_dataset(dataset, bins=64, title='Smiley face dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define a suitable Diffusion Model\n",
    "\n",
    "1. The main requirement for a model capable of parametrizing these tasks is **Output Dimensionality**:\n",
    "    - **The model's output must match the input dimensionality**. Each score corresponds directly to each pixel (or input feature), whether it's predicting gradients, noise values, or a vector field.\n",
    "\n",
    "2. The second requirement is also straightforward an simple to ensure - **the model must take the time step as a conditioning input**.\n",
    "    - The time step provides information about the diffusion/matching process stage, enabling the model to predict the appropriate scores or noise values.\n",
    "\n",
    "3. This means that the **Model Architecture is flexible**. While U-Nets are commonly used for high-dimensional inputs like images, you can use any architecture that supports the task. Recent approaches even use transformers.\n",
    "    - For this notebook, since the distribution we are modelling is not complex, we will use a **Multilayer Perceptron (MLP)** for faster training and inference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroToOneTimeEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, dim: int):\n",
    "        '''\n",
    "        Create a time embedding that maps time to a 2D embedding.\n",
    "        Args:\n",
    "            dim: int: The dimensionality of the embedding\n",
    "        '''\n",
    "        super().__init__()\n",
    "        assert dim % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.register_buffer('freqs', torch.arange(1, dim // 2 + 1) * torch.pi)\n",
    "\n",
    "    def forward(self, t):\n",
    "        '''\n",
    "        Forward pass of the time embedding.\n",
    "        Args:\n",
    "            t: torch.Tensor: The time to embed.\n",
    "        Returns:\n",
    "            torch.Tensor: The embedded time.\n",
    "        '''\n",
    "        emb = self.freqs * t[..., None]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_features, time_embedding_size=8, n_blocks=5):\n",
    "        '''\n",
    "        Create an MLP with residual connections.\n",
    "        Args:\n",
    "            n_features: int: The number of input and output features.\n",
    "            time_embedding_size: int: The size of the time embedding to use.\n",
    "            n_blocks: int: The number of residual blocks to use.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.time_embedding = ZeroToOneTimeEmbedding(time_embedding_size)\n",
    "        hidden_size = n_features + time_embedding_size\n",
    "        blocks = []\n",
    "        for _ in range(n_blocks):\n",
    "            blocks.append(nn.Sequential(\n",
    "                None, # A batch norm layer would go here, taking an input size of `hidden_size`.\n",
    "                None, # A linear layer with hidden_size inputs and outputs.\n",
    "                None, # A ReLU activation.\n",
    "            ))\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.final = None # A linear layer with hidden_size inputs and that outputs n_features.\n",
    "\n",
    "\n",
    "    def forward(self, X, time):\n",
    "        '''\n",
    "        Forward pass of the model.\n",
    "        Args:\n",
    "            X: torch.Tensor: The input features.\n",
    "            time: torch.Tensor: The time to embed.\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the model.\n",
    "        '''\n",
    "        X = torch.cat([X, self.time_embedding(time)], axis=1)\n",
    "        for block in self.blocks:\n",
    "            X = X + block(X)\n",
    "        X = self.final(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: The Forward Diffusion process\n",
    "\n",
    "Ho *et al.* introduced a pivotal shift in how the reverse diffusion process is trained, significantly boosting its computational efficiency. Instead of reconstructing the image at a given timestep, the model is trained to predict the noise added during the forward diffusion process. This change simplifies the training objective and aligns with the mathematical properties of the diffusion process.  \n",
    "\n",
    "In forward diffusion, an input image $x_0$ is progressively corrupted by adding Gaussian noise at each step. The forward distribution for a given timestep \\(t\\) is defined as  \n",
    "\n",
    "$$\n",
    "q(x_t | x_0) = \\mathcal{N}(x_t | \\sqrt{\\bar{\\alpha}_t} x_0, (1 - \\bar{\\alpha}_t) \\mathbf{I}),\n",
    "$$\n",
    "\n",
    "where $\\alpha_t = 1 - \\beta_t, \\quad \\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s.$ This leads to the formulation\n",
    "\n",
    "$$\n",
    "x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon},\n",
    "$$\n",
    "\n",
    "where  $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ represents Gaussian noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardDiffusion:\n",
    "    def __init__(self, beta_1, beta_t, timesteps):\n",
    "        '''\n",
    "        Create a forward diffusion model.\n",
    "        Args:\n",
    "            beta_1: float: The initial value of beta.\n",
    "            beta_t: float: The final value of beta.\n",
    "            timesteps: int: The number of timesteps to use.\n",
    "        '''\n",
    "        self.betas = None # A tensor of shape (timesteps,) ranging from beta_1 to beta_t.\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Hint: use torch.cumprod.\n",
    "        self.bar_alpha = None # A tensor of shape (timesteps,) with the bar_alpha values as defined above.\n",
    "\n",
    "    def sample(self, x, time, noise=None):\n",
    "        '''\n",
    "        Sample from the forward diffusion model.\n",
    "        Args:\n",
    "            x: torch.Tensor: The input to the model.\n",
    "            time: torch.Tensor: The time to sample at.\n",
    "        '''\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x)\n",
    "\n",
    "        # Hint: don't forget that arrays start at 0 in Python.\n",
    "        x_t = None # The x_t value at defined by the forward diffusion model.\n",
    "        return x_t\n",
    "    \n",
    "forward = ForwardDiffusion(beta_1=1e-4, beta_t=0.02, timesteps=100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 6, figsize=(36, 6))\n",
    "\n",
    "plot_dataset(dataset, bins=64, ax=axs[0], title='t = 0', verbose=False)\n",
    "\n",
    "for i in range(20,120,20):\n",
    "    plot_dataset(forward.sample(dataset, i), bins=64, ax=axs[i//20], title=f't = {i}', verbose=False)\n",
    "fig.suptitle('Forward diffusion', fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the Model\n",
    "\n",
    "Instead of directly predicting the reverse mean $\\tilde{\\mu}t$, the model predicts the noise $\\epsilon_\\theta(x_t)$ that corrupts the original data $x_0$. This approach reformulates the problem of learning $\\mu_\\theta(x_t)$ into predicting the noise $\\epsilon$, which simplifies optimization and enhances numerical stability. Thus, the training algorithm is given by\n",
    "\n",
    "1. **repeat**  \n",
    "2. &nbsp;&nbsp;&nbsp;Sample $x_0 \\sim q(x_0)$  \n",
    "3. &nbsp;&nbsp;&nbsp;Sample $t \\sim \\text{Uniform}(\\{1, \\dots, T\\})$  \n",
    "4. &nbsp;&nbsp;&nbsp;Sample $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$  \n",
    "5. &nbsp;&nbsp;&nbsp;Take a gradient descent step on $\\nabla_\\theta \\| \\boldsymbol{\\epsilon} - \\boldsymbol{\\epsilon}_\\theta(\\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\boldsymbol{\\epsilon}, t) \\|^2$  \n",
    "6. **until converged**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_loss(model, forward, batch, device='cpu'):\n",
    "    '''\n",
    "    Perform a single training step on a batch of data.\n",
    "    Args:\n",
    "        model: nn.Module: The model to train.\n",
    "        forward: ForwardDiffusion: The forward diffusion model to use.\n",
    "        batch: torch.Tensor: The batch of data to train on.\n",
    "        device: str: The device to use for training.\n",
    "    Returns:\n",
    "        torch.Tensor: The loss of the model on the batch.\n",
    "    '''\n",
    "    loss_fn = None # The loss function to use, according to step 5 of the algorithm.\n",
    "    time = torch.randint(low=1, high=forward.timesteps+1, size=(batch.shape[0],))\n",
    "    noise = torch.randn_like(batch)\n",
    "\n",
    "    X_t = forward.sample(batch, time, noise=noise)\n",
    "    normalized_time = time.float()/forward.timesteps # the model expects time to be normalized to [0, 1]\n",
    "\n",
    "    X_pred = model(X_t.to(device), normalized_time.to(device))\n",
    "\n",
    "    return loss_fn(X_pred, noise.to(device))\n",
    "\n",
    "def training_loop(model, forward, n_steps, batch_size, lr):\n",
    "    '''\n",
    "    Train a model using the forward diffusion model.\n",
    "    Args:\n",
    "        model: nn.Module: The model to train.\n",
    "        forward: ForwardDiffusion: The forward diffusion model to use.\n",
    "        n_steps: int: The number of training steps to take.\n",
    "        batch_size: int: The number of samples in each batch.\n",
    "        lr: float: The learning rate to use.\n",
    "    '''\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    step_bar = trange(n_steps, desc='Steps')\n",
    "    for step in step_bar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = create_dataset(size=batch_size)\n",
    "        loss = diffusion_loss(model, forward, batch, device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step % 100 == 0:\n",
    "            step_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "model = MLP(n_features=2)\n",
    "training_loop(model, forward, n_steps=2000, batch_size=10000, lr=1e-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Sample from a DDPM\n",
    "\n",
    "Sampling in diffusion models involves generating data by reversing the forward diffusion process. Starting with random Gaussian noise, the model iteratively denoises the sample to produce realistic outputs. At each timestep $t$, the process uses the predicted noise $\\boldsymbol{\\epsilon}_\\theta(x_t, t)$ to compute the intermediate state $x_{t-1}$.\n",
    "\n",
    "1. $x_T \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$  \n",
    "2. **for** $t = T, \\dots, 1$ **do**  \n",
    "3. &nbsp;&nbsp;&nbsp;$z \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ if $t > 1$, else $z = 0$  \n",
    "4. &nbsp;&nbsp;&nbsp;$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\, \\boldsymbol{\\epsilon}_\\theta(x_t, t) \\right) + \\sigma_t z$  \n",
    "5. **end for**  \n",
    "6. **return** $x_0$\n",
    "\n",
    "For the sake of simplicity, we can isolate some elements of this equation:\n",
    "\n",
    "$$ c1 = \\frac{1}{\\sqrt{\\alpha_t}} $$\n",
    "\n",
    "$$ c2 = \\frac{1}{\\sqrt{\\alpha_t}} \\cdot \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} $$\n",
    "\n",
    "so that we have:\n",
    "\n",
    "$$ x_{t-1} = c1 \\cdot x_t - c2 \\cdot \\boldsymbol{\\epsilon}_\\theta(x_t, t) + \\sigma_t z $$\n",
    "\n",
    "\n",
    "The standard deviation $\\sigma_t$ is kept as $\\sqrt{\\beta_t}$, which corresponds to the noise schedule defined during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDiffusionDDPM:\n",
    "    def __init__(self, beta_1, beta_t, timesteps, device='cpu'):\n",
    "        '''\n",
    "        Create a reverse diffusion model.\n",
    "        Args:\n",
    "            beta_1: float: The initial value of beta.\n",
    "            beta_t: float: The final value of beta.\n",
    "            timesteps: int: The number of timesteps to use.\n",
    "            device: str: The device to use for training.\n",
    "        '''\n",
    "        self.betas = None # A tensor of shape (timesteps,) ranging from beta_1 to beta_t (same as in the Forward Diffusion process!).\n",
    "        self.timesteps = timesteps\n",
    "        self.bar_alpha = None # A tensor of shape (timesteps,) with the bar_alpha values (once again, same as in the Forward Diffusion process!).\n",
    "        self.alpha = 1 - self.betas\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()    \n",
    "    def sample(self, n_samples, model):\n",
    "        '''\n",
    "        Sample from the reverse diffusion model.\n",
    "        Args:\n",
    "            n_samples: int: The number of samples to generate.\n",
    "            model: nn.Module: The model to use for sampling.\n",
    "        '''\n",
    "        model.eval()\n",
    "        x_t = torch.randn(n_samples, 2)\n",
    "        x_all = []\n",
    "        x_all.append(x_t)\n",
    "        for i in tqdm(range(self.timesteps, 0, -1)):\n",
    "\n",
    "            c1 = None # The c1 value as defined by the reverse diffusion model.\n",
    "            c2 = None # The c2 value as defined by the reverse diffusion model.\n",
    "            sigma = torch.sqrt(self.betas[i-1]) if i > 1 else 0\n",
    "\n",
    "            # sample noise\n",
    "            noise = torch.randn(n_samples, 2)\n",
    "\n",
    "            # normalize time\n",
    "            t = torch.ones(n_samples)*i/self.timesteps\n",
    "\n",
    "            # determine x_{t-1}\n",
    "            x_t = c1*x_t - c2*model(x_t.to(self.device), t.to(self.device)).cpu().detach() + sigma*noise\n",
    "\n",
    "            x_all.append(x_t)\n",
    "        return x_all\n",
    "    \n",
    "def plot_reverse_diffusion(samples, sample_timesteps):\n",
    "    '''\n",
    "    Plot samples from the reverse diffusion model.\n",
    "    Args:\n",
    "        samples: list: The samples to plot.\n",
    "        sample_tiemsteps: int: The number of timesteps used to sample the data.\n",
    "    '''\n",
    "    fig, axs = plt.subplots(1, 6, figsize=(36, 6))\n",
    "    for i in range(0, sample_timesteps + sample_timesteps//5, sample_timesteps//5):\n",
    "        plot_dataset(samples[i], bins=64, ax=axs[i//(sample_timesteps//5)], title=f't = {sample_timesteps-i}', verbose=False)\n",
    "    fig.suptitle('Reverse diffusion', fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "reverse = ReverseDiffusionDDPM(beta_1=1e-4, beta_t=0.02, timesteps=100)\n",
    "dataset_samples = reverse.sample(100000, model)\n",
    "\n",
    "plot_reverse_diffusion(dataset_samples, reverse.timesteps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Sample With Fewer Steps using a DDIM\n",
    "\n",
    "Song *et al.* [3] introduced Denoising Diffusion Implicit Models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs, that can accelerate sampling by up to 10 times. From $p_\\theta(x_{1:T})$, one can generate a sample $x_{t-1}$ from a given sample $x_t$ as:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\underbrace{\\sqrt{\\bar{\\alpha}_{t-1}} \\left( \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t} \\, \\boldsymbol{\\epsilon}_\\theta(x_t)}{\\sqrt{\\bar{\\alpha}_t}} \\right)}_{\\text{\"predicted } x_0\\text{\"}} \n",
    "+ \\underbrace{\\sqrt{1 - \\bar{\\alpha}_{t-1} - \\sigma_t^2} \\cdot \\boldsymbol{\\epsilon}_\\theta(x_t)}_{\\text{\"direction pointing to } x_t\\text{\"}} \n",
    "+ \\underbrace{\\sigma_t \\boldsymbol{\\epsilon}_t}_{\\text{\"random noise\"}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\epsilon}_t \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ is standard Gaussian noise independent of $x_t$, and $\\bar{\\alpha}_0 := 1$. Different choices of $\\sigma_t$ result in different generative processes, all while using the same model $\\boldsymbol{\\epsilon}_\\theta$, meaning re-training the model is unnecessary.\n",
    "\n",
    "### Special Cases of $\\sigma_t$:\n",
    "1. **DDPM (Denoising Diffusion Probabilistic Model):**  \n",
    "   When $\\sigma_t = \\sqrt{\\frac{1 - \\bar{\\alpha}_{t-1}}{1 - \\bar{\\alpha}_t} \\cdot \\frac{1 - \\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}}}$, the forward process becomes Markovian, and the generative process becomes a standard DDPM.\n",
    "\n",
    "2. **DDIM (Denoising Diffusion Implicit Model):**  \n",
    "   When $\\sigma_t = 0$ for all $t$, the forward process becomes deterministic, meaning that given $x_{t-1}$ and $x_0$, the process is deterministic except at $t=1$. The generative process skips adding random noise, and the coefficient of $\\boldsymbol{\\epsilon}_t$ becomes zero. This results in an *implicit probabilistic model* (as described by Mohamed & Lakshminarayanan, 2016).  \n",
    "\n",
    "   The DDIM retains the DDPM training objective but with a deterministic forward process. Samples are generated from latent variables using a fixed procedure, transitioning from $x_T$ to $x_0$.\n",
    "\n",
    "---\n",
    "\n",
    "#### References\n",
    "<div style=\"font-size: smaller;\">  \n",
    "[3] Song, J., Meng, C., & Ermon, S. Denoising Diffusion Implicit Models. In International Conference on Learning Representations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDiffusionDDIM:\n",
    "    def __init__(self, beta_1, beta_t, timesteps, sample_timesteps, device='cpu'):\n",
    "        '''\n",
    "        Create a reverse diffusion model.\n",
    "        Args:\n",
    "            beta_1: float: The initial value of beta.\n",
    "            beta_t: float: The final value of beta.\n",
    "            timesteps: int: The number of timesteps to use.\n",
    "            sample_timesteps: int: The number of timesteps to sample at.\n",
    "            device: str: The device to use for training.\n",
    "        '''\n",
    "        self.betas = None # A tensor of shape (timesteps,) ranging from beta_1 to beta_t (same as in the Forward Diffusion process!).\n",
    "        self.timesteps = timesteps\n",
    "        self.sample_timesteps = sample_timesteps\n",
    "        self.scaling = self.timesteps // self.sample_timesteps\n",
    "        self.bar_alpha = None # A tensor of shape (timesteps,) with the bar_alpha values (once again, same as in the Forward Diffusion process!).\n",
    "        self.device = device\n",
    "\n",
    "    @torch.no_grad()    \n",
    "    def sample(self, n_samples, model):\n",
    "        '''\n",
    "        Sample from the reverse diffusion model.\n",
    "        Args:\n",
    "            n_samples: int: The number of samples to generate.\n",
    "            model: nn.Module: The model to use for sampling.\n",
    "        '''\n",
    "        model.eval()\n",
    "        x_t = torch.randn(n_samples, 2)\n",
    "        x_all = []\n",
    "        x_all.append(x_t)\n",
    "\n",
    "        # we start at the last timestep and go backwards skipping some steps with the scaling factor\n",
    "        for i in tqdm(range(self.timesteps, 0, -self.scaling)):\n",
    "\n",
    "            if i == self.scaling:\n",
    "                # the last step is not 1, so we need to correct for that\n",
    "                alpha_t_minus_1 = torch.tensor(1.0)\n",
    "            else:\n",
    "                alpha_t_minus_1 = None # The alpha_{t-1} value as defined by the reverse DDIM diffusion model.\n",
    "\n",
    "            # the predicted noise for the current timestep\n",
    "            eps = model(x_t.to(self.device), torch.ones(n_samples)*i/self.timesteps).cpu().detach()\n",
    "\n",
    "            x_0 = None # The x_0 value as defined by the reverse DDIM diffusion model.\n",
    "            x_t_dir = None # The direction pointing to x_t value as defined by the reverse DDIM diffusion model.\n",
    "\n",
    "            # the prediced x_{t-1} value\n",
    "            x_t = x_0 + x_t_dir\n",
    "            x_all.append(x_t)\n",
    "        return x_all\n",
    "\n",
    "reverse = ReverseDiffusionDDIM(beta_1=1e-4, beta_t=0.02, timesteps=100, sample_timesteps=20)\n",
    "dataset_samples = reverse.sample(100000, model)\n",
    "\n",
    "plot_reverse_diffusion(dataset_samples, reverse.sample_timesteps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
