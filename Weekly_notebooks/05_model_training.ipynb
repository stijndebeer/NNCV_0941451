{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59ef229",
   "metadata": {},
   "source": [
    "# Lecture 5: Model Training\n",
    "Welcome to this hands-on tutorial on **training neural networks effectively**. In this notebook, we’ll explore essential concepts and techniques to optimize the training process and improve model performance for a specific application.\n",
    "\n",
    "Training a neural network involves more than just feeding data and adjusting weights—it requires thoughtful strategies to ensure stability, efficiency, and generalization. This module focuses on key elements such as **data normalization**, **weight normalization**, selecting the best **optimizer**, and preventing overfitting through **regularization**.\n",
    "\n",
    "By the end of this notebook, you’ll understand how to:\n",
    "\n",
    "1) Apply data and weight normalization to stabilize and speed up training.\n",
    "2) Choose an appropriate optimizer to efficiently minimize the loss function.\n",
    "3) Implement regularization techniques, such as dropout and weight decay, to reduce overfitting and improve model generalization.\n",
    "\n",
    "Through practical examples and hands-on coding exercises, you’ll gain the skills to fine-tune the training process and achieve better results with your neural networks. Let’s get started and master the art of model training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257494e",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Lets get started for this week by importing the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pdb\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467fcdcc",
   "metadata": {},
   "source": [
    "Let us start with a simple 4 layer network (3 convolutional layers and one fully connected). Even tough the network is simple, it can show us different training concepts within a reasonable amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7274dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleCNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "                    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(64 * 4 * 4, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6cc527",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "Additionally we are going to use the same dataset as you have seen before, CIFAR10. However, now you will pay more attention to the normalization, as you have to implement it yourself. Start with converting the images in the dataset into a tensor. Then normalize the data to ensure optimal training conditions. For this, add the required transformations to the transforms function. It is recommended to have a look at the torchvision documentation.\\\n",
    "Make sure the average statistics of the entire dataset are normalized. ($\\mu=0$ and $\\sigma=1$) Don't forget to check this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ad709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 dataset\n",
    "transform = transforms.Compose([])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "train_subset, val_subset = torch.utils.data.random_split(trainset, [40000, 10000],\n",
    "                                         generator=torch.Generator().manual_seed(1))\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(val_subset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def calculate_mean(dataset):\n",
    "    mean_per_image_r, mean_per_image_g, mean_per_image_b = [], [], []\n",
    "\n",
    "    for image, _ in dataset:\n",
    "        mean_per_image_r.append(torch.mean(image[0,:,:]).tolist())\n",
    "        mean_per_image_g.append(torch.mean(image[1,:,:]).tolist())\n",
    "        mean_per_image_b.append(torch.mean(image[2,:,:]).tolist())\n",
    "\n",
    "    return mean_per_image_r, mean_per_image_g, mean_per_image_b\n",
    "\n",
    "mean_r, mean_g, mean_b = calculate_mean(trainloader.dataset)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.hist([mean_r, mean_g, mean_b], 20, color=[\"red\", \"green\", \"blue\"])\n",
    "plt.title(\"Distribution of the average value per image for each channel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ea2fc",
   "metadata": {},
   "source": [
    "Now it's time to setup a training loop which we use to optimize te model. We also define some additional helper functions for diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, num_epochs=5, lr=0.01, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    epoch_data = collections.defaultdict(list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        epoch_data['loss'].append(epoch_loss)\n",
    "\n",
    "        plot_loss(epoch_data)\n",
    "        plot_weights(model, \"Current Weights\")\n",
    "        \n",
    "\n",
    "def plot_weights(network, title, fignr=1):\n",
    "    fig, ax = plt.subplots(1,3, figsize=(12,3), num=fignr)\n",
    "    i=0\n",
    "    for m in network.modules():\n",
    "        if isinstance(m, nn.Conv2d):            \n",
    "            ax[i].clear()\n",
    "            ax[i].hist(m.weight.detach().numpy().flatten(), color=\"dodgerblue\", bins=40)\n",
    "            ax[i].set_title(\"Conv layer \" + str(i+1) + \"weights\", fontsize=8)\n",
    "            i+=1\n",
    "    plt.suptitle(title, fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(data, title='Training loss'):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for label,data in data.items():\n",
    "        plt.plot(data, label=label, color=\"dodgerblue\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([\"Current loss: \" + str(np.round(data[-1],3))])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c962f498",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "The code below will initialize the layer weights and biases of the network with a constant value. (1 and 0 respectively)\\\n",
    "By running the codeblock you can visualize the weights and watch the training loss.\\\n",
    "Try to improve the training process by choosing other types of layer initialization. Check the pytorch documentation for the possibilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = SimpleCNN()\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.constant_(m.weight, 1) \n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.constant_(m.weight, 1) \n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    # TODO: implement different kind of initalization method\n",
    "\n",
    "# Apply the weights and biases before training\n",
    "simple_cnn.apply(init_weights)\n",
    "\n",
    "# Plot the distribution of the weights\n",
    "plot_weights(simple_cnn, \"Initialized Weights\", fignr=0)\n",
    "\n",
    "# Set hyperparameters\n",
    "nb_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Start model training \n",
    "train_model(simple_cnn, trainloader, nb_epochs, learning_rate, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a347b",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "As we have learned in the lecture, training can be optimized by reducing the covariance shift in the network. We do this by implementing batch normalization in the network. Add a batch normalization layers in the network and observe the training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SimpleCNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    # TODO: Add batchnormalization layers in the CNN model\n",
    "                    \n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc = nn.Linear(64 * 4 * 4, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eafce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = SimpleCNN()\n",
    "\n",
    "# Set hyperparameters\n",
    "nb_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Start model training \n",
    "train_model(simple_cnn, trainloader, nb_epochs, learning_rate, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1486f19",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "**Q1.** Why is normalization such an important concept with model training?\n",
    "\n",
    "**Q2.** Can you explain the concept of Xavier normalization? How does it address the vanishing and exploding gradient problem?\n",
    "\n",
    "**Q3.** Can you explain discuss the benefits of batch normalization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1c758",
   "metadata": {},
   "source": [
    "When optimizing a big model it is always recommended to first experiment with a single image and check if the training loss goes to zero. This is a quick sanity check if the model is able to learning something. Because we already know our model is working, we skip this step for now. But keep this in mind when implementing your own custom models.\n",
    "\n",
    "## Optimization\n",
    "Next we would like to pick the best optimizer. Currently stocastic gradient descent (SGD) is used, but as already explained in the lecture, this is not the most robust solution.\\ Check the documentation which optimizers are available in Pytorch and try them. Note: Diffrences between optimizers are more prevalent with worse network initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=5, lr=0.01, verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ## To do: define your optimizer\n",
    "    optimizer = \n",
    "\n",
    "    epoch_data = collections.defaultdict(list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        epoch_data['loss'].append(epoch_loss)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()      \n",
    "\n",
    "        validation_loss = running_loss / len(val_loader)\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        epoch_data['validation_loss'].append(validation_loss)  \n",
    "\n",
    "        plot_loss(epoch_data)\n",
    "\n",
    "def plot_loss(data, title='Loss'):\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for label,data in data.items():\n",
    "        plt.plot(data, label=label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend([\"Train loss\", \"Validation Loss\"])\n",
    "    plt.show()        \n",
    "\n",
    "# Assign the model        \n",
    "simple_cnn = SimpleCNN()\n",
    "\n",
    "# Set hyperparameters\n",
    "nb_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Start model training \n",
    "train_model(simple_cnn, trainloader, valloader, nb_epochs, learning_rate, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57179d0",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "As you can see in the previous section, te training severely suffers from overfitting. In order to prevent this, regularization is needed.\\\n",
    "Now it is time to try different forms of regularization on the network and check if performance improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c51bb",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "**Q4.** Can you explain the Adam optimizer and its advantages over traditional gradient descent methods?\n",
    "\n",
    "**Q5.** Try to think of other regularization methods and try to implement at least 2 more?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2b853c",
   "metadata": {},
   "source": [
    "## Final Optimization\n",
    "Now you have experienced many important aspects regarding neural network training, it's time to squeeze out every possible bit of performance. We do this by optimizing all hyperparameters. Try different settings for e.g. optimizer parameters such as learning rate, number of epochs, regularization strength. Also have a look at learning rate schedulers.\\\n",
    "Next week we will learn how to properly benchmark the performance of a network by discussing various evaluation metrics. But for now, check the model accuracy on the test set with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1444b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs, labels  # Assuming you're using GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy on the test set: {:.2f}%'.format(accuracy))\n",
    "\n",
    "test_model(simple_cnn, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815563b",
   "metadata": {},
   "source": [
    "You may want to try to optimize a different network architecture. Feel free to pick your own model and apply the things you have just learned!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
